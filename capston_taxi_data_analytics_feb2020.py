# -*- coding: utf-8 -*-
"""Capston_Taxi_Data_Analytics_Feb2020.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/118rE52jrS1ppCQ5WfXPZ7-uxiUFP5odY

# **Data identification and collection**

The data is collected from  https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page

# **Data Cleaning and Manipulation**

The first stage in any EDA problem solving strategy is Data cleaning and Manipulation stage.

**Data Cleaning and Manipulation Steps**

> Data Understanding

- Reading Dataset documentation
- Importing data - understanding each column data
- Data Summarisation like Check data type of columns, number of rows etc.

> Data Cleaning

- Dropping irrelevant columns
- Renaming the columns
- Dropping the duplicate rows 
- Dropping or handling missing values
- Dropping invalid data rows (Also check if column have correct data type)
- Detecting and handling outliers (this can be handled in data analysis part as well)


> Data Manipulation

- Column transformation
- Joining datasets
- other manipulation like pivoting or transposing (this is also applied in  data analysis part)

## Data Understanding

### Reading data documentation

In the website - https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page

there is data about yellow taxis in csv format categorised by each year and month.

There is data dictionary pdf as well mentioned in the website called `Yellow Trips Data Dictionary`

By looking at data_description file we find that following columns would be useful for our analysis:<br>
* tpep_pickup_datetime - The date and time when the meter was engaged. 
* tpep_dropoff_datetime - The date and time when the meter was disengaged.
* Passenger_count - The number of passengers in the vehicle. 
* Trip_distance - The elapsed trip distance in miles reported by the taximeter.
* PULocationID - TLC Taxi Zone in which the taximeter was engaged
* DOLocationID - TLC Taxi Zone in which the taximeter was disengaged
* Payment_type - A numeric code signifying how the passenger paid for the trip. 
* Fare_amount - The time-and-distance fare calculated by the meter.
* Extra - Miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges.
* MTA_tax - \\$ 0.50 MTA tax that is automatically triggered based on the metered rate in use.
* Improvement_surcharge - \\$0.30 improvement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015.
* congestion_surcharge - fees on congestion
* Tip_amount - This field is automatically populated for credit card tips. Cash tips are not included.
* Tolls_amount - Total amount of all tolls paid in trip.
* Total_amount -  The total amount charged to passengers. Does not include cash tips.
* Congestion_surcharge - Total amount collected in trip for NYS congestion surcharge.
* Airport_fee - $1.25 for pick up only at LaGuardia and John F. Kennedy Airports

And we will drop the following columns:<br>
* VendorID
* RateCodeID 
* Store_and_fwd_flag

### Importing the data
"""

# import important libraries - matplotlib, seaborn and pandas
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import random

from google.colab import drive
drive.mount('/content/drive')

# yellow taxi data
file_loc = '/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/yellow_tripdata_Feb_2020.parquet'

# read file
trip_data = pd.read_parquet(file_loc)
trip_data.head()

# taxi zone lookup file
file_loc_2 = '/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/taxi+_zone_lookup.csv'

#zone look up file 
taxi_zone_data = pd.read_csv(file_loc_2)

location_name=taxi_zone_data.loc[taxi_zone_data["LocationID"].isin([186,48,236,237,75,161])]
location_name

"""### Data Summarisation"""

trip_data.head()

trip_data.tail()

print(trip_data.shape)

trip_data.info()

trip_data.describe()

"""## Data Cleaning and Manipulation Steps (Reading Assignment)

We have done all the data cleaning and manipulation steps below though we have not followed the exact cleaning steps methodically as mentioned at the start.

These are the cleaning that we have done
* Dropped 3 columns `'VendorID','RatecodeID','store_and_fwd_flag'`
* converted pickup and dropoff column data type to datetime
* Extracted trip day from the pickup datetime column
* Extracted pickup hour and dropoff hour also from the above datetime columns
* calculated duration value from it
* checked for missing values
* converted payment type value from integer to string (based on mapping given in data dictionary file)
* combined the three tax values (mta_tax , extra, improvement_surcharge) into one single value called total_taxes.
"""

# remove following columns - 'VendorID','RatecodeID','store_and_fwd_flag'
trip_data.drop(['VendorID','RatecodeID','store_and_fwd_flag'],axis=1,inplace=True)
# print data head
trip_data.head()

"""We will now deal with time related columns, we have two time related columns
* tpep_pickup_datetime 
* tpep_dropoff_datetime

We will first convert these column to datatime data type of pandas.

we will create three different features from these  
* hour - pickup hour and dropoff hour
* day name - this is basically the day of the week when trip took place - we will only take day name from pickup date.
( as day name for drop date is supposed to be same as pickup date)
* duration of trip
"""

print(trip_data.info())

## already in datetime format
# # convert 'tpep_pickup_datetime' and 'tpep_dropoff_datetime' to datetime format
# trip_data['tpep_pickup_datetime'] = pd.to_datetime(trip_data['tpep_pickup_datetime'])
# trip_data['tpep_dropoff_datetime'] = pd.to_datetime(trip_data['tpep_dropoff_datetime'])
# # print data info
# print(trip_data.info())
# # print data head
# trip_data.head()

# create 'duration' column using pd.Timedelta(minutes=1)
trip_data['duration'] = (trip_data['tpep_dropoff_datetime'] - trip_data['tpep_pickup_datetime'])/ pd.Timedelta(minutes=1)
# create 'trip_pickup_hour' column using 'tpep_pickup_datetime' column
trip_data['trip_pickup_hour'] = trip_data['tpep_pickup_datetime'].dt.hour
# create 'trip_dropoff_hour' column using 'tpep_dropoff_datetime' column
trip_data['trip_dropoff_hour'] = trip_data['tpep_dropoff_datetime'].dt.hour
# create 'trip_day' column using 'tpep_pickup_datetime' column - use day_name()
trip_data['trip_day'] = trip_data['tpep_pickup_datetime'].dt.day_name()
# print data info
# print(trip_data.info())
# print data head
trip_data.head()

"""Let's also see the number of missing values for each column"""

# print missing values for each column - use .isnull().sum
trip_data.isnull().sum(axis=0).reset_index()

"""* From the above table we can observe missing values in passenger_count and congestion_surcharge for 48834 trips.

* Let us check this missing data
"""

passenger_null_data=trip_data.loc[trip_data["passenger_count"].isnull()]
passenger_null_data

passenger_null_data["payment_type"].value_counts()

"""* For the trips, where passesnger count is NaN, there is no payment data available
* To do further analysis we dropping these rows from the trip_data, this is around 0.78% of total trips
"""

trip_data=trip_data.loc[trip_data["passenger_count"].notnull()]
trip_data.isnull().sum(axis=0).reset_index()

"""* Now there are no NaN values in passenger count, we can go ahead

For payment_type we have the following mapping for categories:<br>
1= Credit card
2= Cash
3= No charge
4= Dispute
5= Unknown
6= Voided trip

let's just check if we have only these categories available in payment_type or not
"""

# value_counts for 'payment_type' column
trip_data['payment_type'].value_counts()

"""Now we will replace these number in payment category with actual category names."""

# function for mapping numerical payment_type to actual payment
def map_payment_type(x):
    if x==1:
        return 'Credit_card'
    elif x==2:
        return 'Cash'
    elif x==3:
        return 'No_charge'
    elif x==4:
        return 'Dispute'
    elif x==5:
        return 'Unknown'
    else:
        return 'Voided_trip'

# use .apply and lambda on payment_type column to change 'payment_type' column
trip_data['payment_type'] = trip_data.payment_type.apply(lambda x:map_payment_type(x))
# print data head
trip_data.head()

# print data info to show that payment_type data type has changed
trip_data.info()

print(trip_data["airport_fee"].unique())
print(trip_data["airport_fee"].value_counts())

"""* It can be observed that, airport_fee is empty, this means neither any trips were taken to airport nor from airport
* So for further analysis we dropping `airport_fee` column
"""

trip_data.drop(["airport_fee"],inplace=True,axis=1)

trip_data.head()

"""Now our Total_amount is basically<br>
Total_amount = fare_amount + tolls_amount + tip_amount + (extra + mta_tax + improvement_surcharge + congestion_surcharge)

of the above components of total_amount we will specifically focus on 'fare_amount','tip_amount', 'tolls_amount' and 'total taxes'.

We are combining the extra, mta_tax, improvement_surcharge and congestion_surcharge under one category called total_taxes as these are determined by local laws and taxes and is not dependent upon distance travelled or time taken for trip.

Here total taxes would be the sum of three columns 'extra','mta_tax', 'improvement_surcharge', 'congestion_surcharge'. So we will make a new column for total_taxes.

We will also drop these three columns 'extra','mta_tax','improvement_surcharge'.

"""

# create 'total_taxes' column from summing 'extra','mta_tax', 'improvement_surcharge', 'congestion_surcharge'
trip_data['total_taxes'] = trip_data['extra']+trip_data['mta_tax']+trip_data['improvement_surcharge']+trip_data['congestion_surcharge']
# drop 'extra','mta_tax','improvement_surcharge', 'congestion_surcharge' columns
trip_data.drop(['extra','mta_tax','improvement_surcharge','congestion_surcharge'],axis=1,inplace=True)
# print data head
trip_data.head()

trip_data.info()

trip_data.to_parquet('/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/yellow_tripdata_Feb_2020_cleaned.parquet',index=False)

"""We will be reading back this file in data analysis and visualisation step.

# Data Analysis and Visualisation

Let's load back our cleaned dataset here.
"""

# import matplotlib.pyplot as plt
# import seaborn as sns
# import pandas as pd

trip_data = pd.read_parquet('/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/yellow_tripdata_Feb_2020_cleaned.parquet')
trip_data.head()

trip_data.info()

"""Now this problem statement is very open ended as we are not given a clear idea exactly what kinds of analysis we should do.

We are just told about the data which we have to explore and analyse.

In such cases we could follow the below series of steps to standardise our exploration of the dataset.

- Start by writing the assumption and question that you want to solve (This is just a preliminary assumption and question writing and you need to be too much detailed here). The purpose of this is to identify the most important analysis that you should and not miss it out.
- Univariate analysis
    - Analyse each feature separately
    - Depending upon data type execute the analysis
     - numerical and visual
    - Data validation - check if the data is correct or not
    - You should try to identify trends and test your assumptions here 
    - while doing this analysis try to identify the analysis points helping in solving the question in first steps
- Bivariate analysis 
    - analyse combination of features (if possible analyse all combinations but definitely go for the feature combinations that makes most sense to analyse)
    - correlation and pair plotting
    - You should try to identify trends and test your assumptions here
    - while doing this analysis try to identify the analysis points helping in solving the question in first steps
- Custom Analysis - You would have some insights by this point of the analysis. Then you deep dive into these insights.
- Combine your results and present it

Except for custom analysis part we will be executing all the parts below.

## ASSUMPTIONS/ANALYSIS THAT MIGHT BE USEFUL<br>

**IMPORTANT CHARACTERISTICS OF A TRIP**
* fare_amount, trip_distance, duration and passenger count distribution can tell us about the important characteristics about the trip.


**PRICING EXPLORATION**
* payment_type can tell us which kind of payment mode the customer usually favours.


* Another issue that taxi companies face is pricing the trip appropriately. So for exploring the pricing of trip, we can also look into the relationship between pricing related variables and hour/day of trip and pricing related variables and location.

**TIME/LOCATION EXPLORATION** 
* To maximize the earnings we should be focussing on trips which are on busy locations and busy times. 


**DURATION OF TRIP EXPLORATION**
* A typical taxi company faces a common problem of efficiently assigning the cabs to passengers so that the service is smooth and hassle free. One of main issue is determining the duration of the current trip. So, We should look into relationship between duration and location, duration and hour of trip.

## **Univariate Analysis**

The first step in doing any kind of EDA is identifying the distribution of important variables in EDA. This helps us in finding important insights about the data.<br>
We should look into the distribution of these specific columns:<br>
Price Based Columns
* fare_amount
* tip_amount
* total_taxes
* tolls_amount
* payment_type
* total_amount

Time Based Columns
* duration
* trip_pickup_hour
* trip_dropoff_hour
* trip_day

Distance/Location Based Columns
* trip_distance
* PULocationID
* DOLocationID

Other columns
* passenger_count

Before we explore the distribution of each column we must identify column category because distribution analysis depends upon variable category:<br>
* Continuous - column which are measurable and uncountable in nature - we use histograms and box plot
* Categorical - column which have categories as it data - we use bar charts

Following columns are continuous in nature:<br>
* fare_amount
* tip_amount
* total_taxes
* total_amount
* duration
* trip_distance
* tolls_amount

Following columns are categorical in nature:<br>
* payment_type
* trip_pickup_hour - it has 24 categories
* trip_dropoff_hour - it has 24 categories
* trip_day - it has 7 categories
* PULocationID
* DOLocationID
* Passenger_count

We will look into the distrbution of passenger_count at the last.

**CONTINUOUS VARIABLE DISTRIBUTION**
"""

# continuous_columns list
continuous_columns = ['fare_amount','tip_amount','total_taxes','total_amount','duration','trip_distance','tolls_amount']

trip_data[continuous_columns].head()

# use .describe() for showing the statistics for continuous columns
trip_data[continuous_columns].describe()

"""Since we are trying to understand the distribution of continuous numerical variables, we will be using 
* histograms
* box plots

Below we have used a for loop to loop through all the continuous variables and then draw histograms and box plots for each of them at each iteration.
"""

# for loop for continuous_columns variable
for feature in continuous_columns:
    fig,ax = plt.subplots(1,2,figsize=(12,6))
    ax[0].hist(trip_data[feature])
    ax[0].set_title('histogram of column values in '+feature)
    sns.boxplot(trip_data[feature],ax=ax[1])
    # using ax2.set_title for box plot
    ax[1].set_title('box plot of column values in '+feature)
    # seaborn style setting
    sns.set()
    # matplotlib command for displaying plots
    plt.show()

"""Negtive values for columns does not make sense<br>
fare_amount<br>
tip_amount<br>
total_taxes<br>
tolls_amount<br>
total_amount<br>
duration<br>

Let's just observe how the negative values in each of these columns look like
"""

# using .loc to show negative values in fare_amount  # 8 mil rows
trip_data.loc[trip_data['fare_amount']<0]

# using .loc to show negative values in tip_amount
trip_data.loc[trip_data['tip_amount']<0]

# using .loc to show negative values in tolls_amount
trip_data.loc[trip_data['tolls_amount']<0]

# using .loc to show negative values in total_taxes
trip_data.loc[trip_data['total_taxes']<0]

# using .loc to show negative values in total_amount
trip_data.loc[trip_data['total_amount']<0]

"""* Can i replace negative values of fare_amount with 0?

* Ans. Not here. Because it will not give us a good measure of averages or other statistical measures.

From the above table displays it is clear whenever fare_amount is negative, we have negative values in 'tip_amount','total_taxes' and 'total_amount'. You can check this by looking at the indexes of few of the rows.


Negative values for these cases does not make sense for doing our analysis. The reason for these negative values can be explored later on if we want to understand the data more better. For now we will remove these rows. 

Also, number of negative rows are 19983 which is 0.32% of total 6299367 observations. So even if we remove them it does not hamper the quantity of data that we have.
"""

# data shape before filtering negative fare_amount rows
print(trip_data.shape)
# using .loc to filter only those rows where fare_amount is positive 
trip_data = trip_data.loc[trip_data['fare_amount']>=0]
# print data shape
print(trip_data.shape)
# print data.head()
trip_data.head()

"""Let's check if removing rows with negative fare amount has also removed rows with negative tip_amount, total_taxes etc."""

print(trip_data.loc[trip_data['tip_amount']<0].shape)
print(trip_data.loc[trip_data['total_taxes']<0].shape)
print(trip_data.loc[trip_data['tolls_amount']<0].shape)

"""* we can see tolls still have one negative values, let us drop this row as well"""

# data shape before filtering negative tolls_amount rows
print(trip_data.shape)
# using .loc to filter only those rows where tolls_amount is positive 
trip_data = trip_data.loc[trip_data['tolls_amount']>=0]
# print data shape
print(trip_data.shape)
# print data.head()
trip_data.head()

"""We will look into the negative values for duration"""

# using .loc to show negative values in duration
trip_data.loc[trip_data['duration']<0]

"""Since there are only two rows with negative duration, we will remove them so as to do our analysis in a better way"""

# using .loc to filter only those rows where duration is positive 
print(trip_data.shape)
trip_data = trip_data.loc[trip_data['duration']>=0]
print(trip_data.shape)

"""Now we will again look at the distribution plots for these variables"""

# for loop for continuous_columns variable
for feature in continuous_columns:
    fig,ax = plt.subplots(1,2,figsize=(12,6))
    ax[0].hist(trip_data[feature])
    ax[0].set_title('histogram of column values in '+feature)
    sns.boxplot(trip_data[feature],ax=ax[1])
    # using ax2.set_title for box plot
    ax[1].set_title('box plot of column values in '+feature)
    # seaborn style setting
    sns.set()
    # matplotlib command for displaying plots
    plt.show()

# use .describe() again to show the statistics for these continuous variables
trip_data[continuous_columns].describe()

"""we need to improve the look of histograms and box plots further as we are not able to clearly observe the distribution.

We will filter all the data for each feature with values less than 90% ile. Then plot that data as shown below
"""

# for loop for continuous_columns variable
for feature in continuous_columns:
    # removing the outliers
    feature_data_percentile = trip_data[feature].quantile(0.95)
    feature_data = trip_data.loc[trip_data[feature]<feature_data_percentile,feature]
    fig,ax = plt.subplots(1,2,figsize=(12,6))
    ax[0].hist(feature_data)
    ax[0].set_title('histogram of column values in '+feature)
    sns.boxplot(feature_data,ax=ax[1])
    # using ax2.set_title for box plot
    ax[1].set_title('box plot of column values in '+feature)
    # seaborn style setting
    sns.set()
    # matplotlib command for displaying plots
    plt.show()

trip_data.nlargest(2,"fare_amount")

trip_data.nlargest(2,"tolls_amount")

trip_data.nlargest(2,"tip_amount")

trip_data.nlargest(2,"duration")

trip_data.nlargest(2,"trip_distance")

"""Looking from the above histograms and box plots we can decipher following information for each column <br>
* fare_amount  - most of the fare amount is within 10 dollar value as is shown by the median value. Though there are some significant outliers, the maximum of which is 6052 dollars.


* tip_amount - most of the tip amount is within 0-3 dollar as is shown by the median value. Though again here too we have outliers, the maximum of which is around 549 dollars. 


* total_taxes - most of the total_taxes values is within 3-6 dollars as is shown by the median value. Though we have outliers in this case but it is not as signiificant as the case for tip and fare.


* total_amount - most of the total_amount values is within 10-20 dollars as is shown by the median value. Again the outliers in this case seems mostly because of outliers in fare_amount.

* tolls_amount - most of the tolls_amount value is 0 so it seems most of the trips do not have to pay for tolls. Still there are outliers and the maximum toll amount is 926 dollars


* duration - most of the values in duration is within 5-16 minutes range as is shown by the median value. We do have some outliers which are beyond the range of 3000 minutes.


* trip_distance - most of the trip_distance is within 1-2.5 miles value as is shown by the median. Outliers in this case seems mostly because of outliers in duration

**CATEGORICAL VARIABLE DISTRIBUTION**<br>
Let's move on to analyse the distribution of categorical variables

for analysing distribution of categorical variables we use bar plots showing the count% of each category.
"""

# list of categorical_variables
categorical_variables = ['payment_type','trip_pickup_hour','trip_dropoff_hour','trip_day','PULocationID','DOLocationID']

# start exploration with payment_type using .value_counts()
trip_data['payment_type'].value_counts()

# but this is a series for ease of plotting we need to use dataframe using .reset_index() on value_counts()
payment_type_category_count = trip_data['payment_type'].value_counts().reset_index()
# print the above dataframe
payment_type_category_count

# we are shown the count under each category but it is better to have count% for comparison - create count_percent col
payment_type_category_count['count_percent'] = (payment_type_category_count['payment_type']/trip_data.shape[0])*100
# print the data frame
payment_type_category_count

# now let's plot it as bar chart
# first step - create fig, ax object using plt.subplots
fig,ax = plt.subplots(figsize=(7,7))
# second step - use sns.barplot(x, y , data, ax) for plotting bar plot
sns.barplot(x = 'index', y = 'count_percent', data=payment_type_category_count,ax=ax)
# third step - use ax object to change plot properties - here we set a title with ax.set_title()
ax.set_title('box plot for payment_type column')
# third step - seaborn style setting
sns.set()
# fourth step - use plt.show() for showing the plots
plt.show()

"""* From above we can understand that most of the payments are done through cash and credit cards. The proportion of credit card payments is around 75% and for cash is around 24%.

Now we look into time based categorical variables.<br>
* 'trip_pickup_hour'
* 'trip_dropoff_hour'
* 'trip_day'

"""

# now let's plot all the time based categorical variables in this way using a for loop
for feature in ['trip_pickup_hour','trip_dropoff_hour','trip_day']:
    # Create a dataframe for the feature using value_counts().reset_index()
    feature_value_counts = trip_data[feature].value_counts().reset_index()
    # create count_percent column 
    feature_value_counts['count_percent'] = (feature_value_counts[feature]/trip_data.shape[0])*100
    # print the number of categories in the feature
    print('Number of categories in feature '+ feature + ' is ' + str(feature_value_counts.shape[0]))
    # Create fig,ax object using plt.subplots 
    if feature_value_counts.shape[0]<10:
        fig,ax = plt.subplots(figsize=(7,7))
    else:
        fig,ax = plt.subplots(figsize=(20,7))
    # plot barplot x='index' and y='count_percent' using sns.barplot
    sns.barplot(x='index',y='count_percent',data=feature_value_counts,ax=ax)
    # set_title
    ax.set_title('Bar plot for '+ feature)
    # set_xlabel
    ax.set_xlabel(feature)
    sns.set()
    plt.show()

"""Based on above plots we can observe following things
* Trip Hour 
    * The dropoff and pick up hour distribution looks almost same
    * Peak hour for the pick up and drop off is around evening from 5 to 7. The busiest time is 6PM.
    * there is less traffic during night times and only after 8AM in morning does the pickup and drop off starts picking up pace.
* Trip day
    * Saturday has the highest taxi uses while Sunday has the lowest taxi uses.
    * From Monday to Friday taxi uses increases continuously.

Moving on we will explore the distribution of location based features:<br>
* 'PULocationID'
* 'DOLocationID'
"""

# let's see the number of categories available in both pickup and dropoff location - PULocationID and DOLocationID
print(trip_data['PULocationID'].value_counts().shape)
print(trip_data['DOLocationID'].value_counts().shape)

"""So we have around 260 categories for location. To plot it on bar plots we need to increase the figure size.

"""

for feature in ['PULocationID','DOLocationID']:
    # Create a dataframe for the feature using value_counts().reset_index()
    feature_value_counts = trip_data[feature].value_counts().reset_index()
    # create count_percent column 
    feature_value_counts['count_percent'] = (feature_value_counts[feature]/trip_data.shape[0])*100
    # print the number of categories in the feature
    print('Number of categories in feature '+ feature + ' is ' + str(feature_value_counts.shape[0]))
    # Create fig,ax object using plt.subplots 
    fig,ax = plt.subplots(figsize=(25,7))
    # plot barplot x='index' and y='count_percent' using sns.barplot
    g=sns.barplot(x='index',y='count_percent',data=feature_value_counts,ax=ax)
    # set_title
    g.set_title('Bar plot for '+ feature)
    # set_xlabel
    g.set_xlabel(feature)
    g.set_xticklabels(labels=list(feature_value_counts.index),rotation='vertical')
    sns.set()
    plt.show()

"""The above plots looks quite messy but one insight that we can indetify from above plot that most of pickup and dropoff points do not have more than 1% traffic (1 percent of 6299367 total trips is 62994).

So in our next plot we will filter out these pickup and dropoff points to look into the graph more clearly.
"""

for feature in ['PULocationID','DOLocationID']:
    feature_value_counts = trip_data[feature].value_counts().reset_index()
    feature_value_counts['count_percent'] = (feature_value_counts[feature]/trip_data.shape[0])*100
    # filter only those location which has more than 0.5 % of traffic
    feature_value_counts = feature_value_counts.loc[feature_value_counts['count_percent']>=1]
    print('Number of categories in feature '+ feature + ' above 1 % count is ' + str(feature_value_counts.shape[0]))
    fig,ax = plt.subplots(figsize=(25,7))
    sns.barplot(x='index',y='count_percent',data=feature_value_counts,ax=ax)
    ax.set_title('Bar plot for '+ feature)
    ax.set_xlabel(feature)
    sns.set()
    plt.show()

"""From the above plots we can glance following insights<br>
* The busiest location in terms of pickup are 161, 236 and 237
* The busiest location for dropoff too are 161, 236 and 237.

We can also look for routes which are busiest. 

For exploring busy routes we need to create a new route column which is a combination of pickup and dropoff point.

So route = 'PULocationID'-'DULocationID'
"""

print(trip_data['PULocationID'])
print(trip_data['DOLocationID'])

trip_data.to_parquet('/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/yellow_tripdata_Feb_2020_cleaned_before_routes.parquet',index=False)

trip_data=pd.read_parquet('/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/yellow_tripdata_Feb_2020_cleaned_before_routes.parquet')

# create routes column using PULocationID and DOLocationID with lambda function
# trip_data['routes'] = trip_data.apply(lambda x: (str(x['PULocationID'])+'-'+str(x['DOLocationID'])),axis=1)

"""Since the above code takes a lot of time to execute we will import already created routes data based on the above code and then merge it with the trip_data dataframe."""

# print the first five rows of routes data
# trip_data['routes'].head()

# save routes data to csv to load it later for analysis
# trip_data['routes'].to_csv('/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/yellow_tripdata_Feb_2020_cleaned_after_routes.parquet',index=False)

# loading routes_data
file_loc_routes_data = '/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/yellow_tripdata_Feb_2020_cleaned_after_routes.parquet'
routes_data = pd.read_csv(file_loc_routes_data)
# assigning new column 'routes' in trip_data to routes_data
trip_data['routes'] = routes_data

trip_data.head()

"""Now let's explore routes through the same bar plot code that we used for Location ID's. But in this case we will only look for routes with more than 0.25% counts (21889 trips)."""

sns.set_style("darkgrid")

# plot bar plot for routes which have trip count above 0.25%
feature = 'routes'
feature_value_counts = trip_data[feature].value_counts().reset_index()
feature_value_counts['count_percent'] = (feature_value_counts[feature]/trip_data.shape[0])*100
# choosing routes where the trip percent is above 0.25% of total trips
feature_value_counts = feature_value_counts.loc[feature_value_counts['count_percent']>=0.25]
print('Number of categories in feature '+ feature + ' above 0.25 % count is ' + str(feature_value_counts.shape[0]))
fig,ax = plt.subplots(figsize=(25,7))
sns.barplot(x='index',y='count_percent',data=feature_value_counts,ax=ax)
ax.set_title('Bar plot for '+ feature)
ax.set_xlabel(feature)
sns.set()
plt.show()

"""From the above plot we can observe that 5 busiest route are following:<br>
237-236<br>
236-236<br>
236-237<br>
237-237<br>
264-264<br>

Finally we will look into the distribution of passenger_count
"""

# look into value_counts of 'passenger_count'
passenger_counts_trip=trip_data['passenger_count'].value_counts()
passenger_counts_trip

passenger_counts_trip_per=(passenger_counts_trip/(list(trip_data.shape))[0])*100
passenger_counts_trip_per

"""Here we see that the mostly 1 or 2 passengers avail the cab. The instance of large group of people travelling together is rare.

This is the result of some of the most important insights after doing univariate analysis:<br>

* fare_amount  - most of the fare amount is within 10 dollar value as is shown by the median value. Though there are some significant outliers, the maximum of which is 6052 dollars.

* tip_amount - most of the tip amount is within 0-3 dollar as is shown by the median value. Though again here too we have outliers, the maximum of which is around 549 dollars. 

* total_taxes - most of the total_taxes values is within 3-6 dollars as is shown by the median value. Though we have outliers in this case but it is not as signiificant as the case for tip and fare.

* total_amount - most of the total_amount values is within 10-20 dollars as is shown by the median value. Again the outliers in this case seems mostly because of outliers in fare_amount.

* tolls_amount - most of the tolls_amount value is 0 so it seems most of the trips do not have to pay for tolls. Still there are outliers and the maximum toll amount is 926 dollars

* duration - most of the values in duration is within 5-16 minutes range as is shown by the median value. We do have some outliers which are beyond the range of 3000 minutes.

* trip_distance - most of the trip_distance is within 1-2.5 miles value as is shown by the median. Outliers in this case seems mostly because of outliers in duration

* Most of the payments are done through cash and credit cards. The proportion of credit card payments is around 75% and for cash is around 24%.

* Trip Hour 
    * The dropoff and pick up hour distribution looks almost same
    * Peak hour for the pick up and drop off is around evening from 5 to 7. The busiest time is 6PM.
    * there is less traffic during night times and only after 8AM in morning does the pickup and drop off starts picking up pace.

* Trip day
    * Saturday has the highest taxi uses while Sunday has the lowest taxi uses.
    * From Monday to Friday taxi uses increases continuously.

* The busiest location in terms of pickup are 161, 236 and 237

* The busiest location for dropoff too are 161, 236 and 237.

* Mostly 1 or 2 passenger avail the cab. Group rides are less common.

## **Bivariate Analysis**

Remember that we made some analysis points regarding exploration of duration and pricing:<br>

For pricing we will be exploring it's relationship with:<br>
* hour/day of trip 
* pickup location of trip

For duration we will be exploring it's relationship with:<br>
* hour of day 
* pickup location of trip

**PRICING EXPLORATION**

We have following variables in the dataset that is associated with pricing:<br>
* fare_amount
* tip_amount
* total_taxes
* tolls_amount
* total_amount

In our anlaysis for now we will be focussing on:<br>
* fare_amount
* tip_amount
* total_taxes
* total_amount

we are leaving tolls_amount for now from our analysis as it contributes very little to the total_amount value because it's median value was 0 i.e. most of the trips are not paying tolls_amount.


*** PRICING VARIABLE EXPLORATION WITH HOUR/DAY OF TRIP ***<br>
All of our pricing variables are continuous and Hour/Day is categorical.

The way to explore relationship between a continuous variable and categorical variable is through a box plot. We create box plot for each category of categorical variable so as to see how the distribution changes for the continuous variables as the category values changes for categorical variable.

We will start with fare_amount exploration.

Let's do a box plot of fair_amount with hour/day of trip to see how the fare changes for different hours of the day and for different days of the week
"""

# fig,ax object using plt.subplots()
fig,ax = plt.subplots(figsize=(25,7))
# box plot using - sns.boxplot(x, y , data, ax)
sns.boxplot(x = 'trip_pickup_hour',y='fare_amount',data=trip_data,ax=ax)
# ax.set_title
ax.set_title('box plot of fare_amount wrt hour of the day')
# seaborn style setting
sns.set()
# matplotlib plt.show()
plt.show()

# fig,ax object using plt.subplots()
fig,ax = plt.subplots(figsize=(25,7))
# box plot using - sns.boxplot(x, y , data, ax)
sns.boxplot(x = 'trip_dropoff_hour',y='fare_amount',data=trip_data,ax=ax)
# ax.set_title
ax.set_title('box plot of fare_amount wrt hour of the day')
# seaborn style setting
sns.set()
# matplotlib plt.show()
plt.show()

"""* From the above plot we can observe that most of the outliers in fare happens at 15 or 3PM to 19 or 7PM based on pickup time and dropoff time.

* For observing the distribution in a better way we would restrict the fare_amount to below 50 dollars. 
"""

# trip_data = trip_data.loc[trip_data['fare_amoun']]

# restricted_fare_amount_data dataframe formation by filtering fare_amount less than 50 dollars
restricted_fare_amount_data = trip_data.loc[(trip_data['fare_amount']<=50) & (trip_data['fare_amount']>=0)]
restricted_fare_amount_data.head()

fig,ax = plt.subplots(figsize=(25,7))
sns.boxplot(x = 'trip_pickup_hour',y='fare_amount',data=restricted_fare_amount_data,ax=ax)
ax.set_title('box plot of fare_amount wrt hour of the day')
sns.set()
plt.show()

fig,ax = plt.subplots(figsize=(25,7))
sns.boxplot(x = 'trip_dropoff_hour',y='fare_amount',data=restricted_fare_amount_data,ax=ax)
ax.set_title('box plot of fare_amount wrt hour of the day')
sns.set()
plt.show()

"""* We can see from the plots that trip pickup and dropoff hours do not have much affect on median fare_amount as median is almost same for all the hours.
* Though there is variability in during 3:00 - 6:00 AM.

let's us see if hour of day has any effect on other pricing related variables or not.

Starting with total_amount
"""

fig,ax = plt.subplots(figsize=(25,7))
# sns.boxplot changes
sns.boxplot(x = 'trip_pickup_hour',y='total_amount',data=trip_data,ax=ax)
ax.set_title('box plot of total_amount wrt hour of the day')
sns.set()
plt.show()

fig,ax = plt.subplots(figsize=(25,7))
# sns.boxplot changes
sns.boxplot(x = 'trip_dropoff_hour',y='total_amount',data=trip_data,ax=ax)
ax.set_title('box plot of total_amount wrt hour of the day')
sns.set()
plt.show()

"""Again here since we are plotting full range of total_amount our graph is able to show heavy outliers prominently but not the distribution of general cases.

So we will again build a dataframe for total_amount with restricted values less than 50 dollars
"""

# restricted_total_amount_data for filtering total_amount data to less than 50 dollars
restricted_total_amount_data = trip_data.loc[trip_data['total_amount']<=50]
restricted_total_amount_data.shape

fig,ax = plt.subplots(figsize=(25,7))
sns.boxplot(x = 'trip_pickup_hour',y='total_amount',data=restricted_total_amount_data,ax=ax)
ax.set_title('box plot of total_amount wrt hour of the day')
sns.set()
plt.show()

fig,ax = plt.subplots(figsize=(25,7))
sns.boxplot(x = 'trip_dropoff_hour',y='total_amount',data=restricted_total_amount_data,ax=ax)
ax.set_title('box plot of total_amount wrt hour of the day')
sns.set()
plt.show()

"""Again we can see the median value does not changes much for each hour though there is variability in price across the hours indicated by different sizes of boxes for different hours.

We will explore tip_amount and total_taxes now. But for exploring them we will retrict the values for these variables to below 10 dollars because the median value for tip_amount was around 1-2 dollars while for total_taxes was around 0.8 dollars so if to see the general distribution clearly we are restricting it to a range of 5 times the median value.
"""

restricted_tip_amount_data = trip_data.loc[trip_data['tip_amount']<10]
restricted_total_taxes_data = trip_data.loc[trip_data['total_taxes']<10]

fig,ax = plt.subplots(figsize=(25,7))
sns.boxplot(x = 'trip_pickup_hour',y='tip_amount',data=restricted_tip_amount_data,ax=ax)
ax.set_title('box plot of tip_amount wrt hour of the day')
sns.set()
plt.show()

fig,ax = plt.subplots(figsize=(25,7))
sns.boxplot(x = 'trip_dropoff_hour',y='tip_amount',data=restricted_tip_amount_data,ax=ax)
ax.set_title('box plot of tip_amount wrt hour of the day')
sns.set()
plt.show()

"""* Based on tip_amount plot we can see that tip_amount too does not vary much based on hours.

Let's observe total_taxes now
"""

# total_taxes = extra + improvement_surcharges + Mta

fig,ax = plt.subplots(figsize=(25,7))
sns.boxplot(x = 'trip_pickup_hour',y='total_taxes',data=restricted_total_taxes_data,ax=ax)
ax.set_title('box plot of total_taxes wrt hour of the day')
sns.set()
plt.show()

fig,ax = plt.subplots(figsize=(25,7))
sns.boxplot(x = 'trip_dropoff_hour',y='total_taxes',data=restricted_total_taxes_data,ax=ax)
ax.set_title('box plot of total_taxes wrt hour of the day')
sns.set()
plt.show()

"""Now in this plot we can clearly observe that total_taxes change with hour of the day. There are two patterns that we can observe here:<br>
* from the hour 8PM to 5AM, their are outliers and it may be due to some overnight surcharges.
* Evening from 4PM to 7PM have quite variable taxes and is a bit higher than other times, it may be due to higher traffic charges.

Overall the effect of hour of day is most clearly visible on total_taxes. we have two insights about how taxes change with hours
* Overnight charges are applied between 8PM to 5AM
* Evening has higher variability in taxes and the taxes are usually high.

Let's move and explore the distribution of pricing variables with respect to day of week. For this analysis we will be using restricited version of dataset that we built for fare_amount, total_amount, tip_amount and total_taxes.
"""

# plot of trip_day with fare_amount
fig,ax = plt.subplots(figsize=(7,7))
# changes in sns.boxplot x and y
sns.boxplot(x = 'trip_day',y='fare_amount',data=restricted_fare_amount_data,ax=ax)
ax.set_title('box plot of fare_amount wrt the day of the week')
sns.set()
plt.show()

fig,ax = plt.subplots(figsize=(7,7))
sns.boxplot(x = 'trip_day',y='total_amount',data=restricted_total_amount_data,ax=ax)
ax.set_title('box plot of total_amount wrt the day of the week')
sns.set()
plt.show()

fig,ax = plt.subplots(figsize=(7,7))
sns.boxplot(x = 'trip_day',y='tip_amount',data=restricted_tip_amount_data,ax=ax)
ax.set_title('box plot of tip_amount wrt the day of the week')
sns.set()
plt.show()

fig,ax = plt.subplots(figsize=(7,7))
sns.boxplot(x = 'trip_day',y='total_taxes',data=restricted_total_taxes_data,ax=ax)
ax.set_title('box plot of total_taxes wrt the day of the week')
sns.set()
plt.show()

"""We can see that mean pricing overall does not change much with respect to day of week.

*** PRICING VARIABLE EXPLORATION WITH LOCATION OF TRIP ***<br>

Here we will look into the price changes for the most frequent trip pickup locations.
"""

# create a new series using value_counts() on 'PULocationID'
pickup_location_value_counts = trip_data['PULocationID'].value_counts()
# show the series
pickup_location_value_counts.head()

# top 10 frequent pickup locations using .nlargest(10).index
top_10_frequent_pickup_locations = pickup_location_value_counts.nlargest(10).index
top_10_frequent_pickup_locations

# for loop for plotting box plot of each of the top 10 frequent pickup locations
for top_pickup_locID in top_10_frequent_pickup_locations:
    # create the new dataframe for each location using .loc on 'PULocationID' - pickup_locID_dataframe
    pickup_locID_dataframe = trip_data.loc[trip_data['PULocationID'] == top_pickup_locID]
    # print the median fare_amount for the top_pickup_locID
    print('The median fare_amount of trips taken from '+str(top_pickup_locID)+' is '+str(pickup_locID_dataframe['fare_amount'].median()))
    # fig,ax object
    fig,ax = plt.subplots(figsize=(6,6))
    # sns.boxplot of fare_amount from the dataframe pickup_locID_dataframe
    sns.boxplot(pickup_locID_dataframe['fare_amount'],ax=ax)
    # set_title
    ax.set_title('box plot of fare_amount for pickup location '+ str(top_pickup_locID))
    sns.set()
    plt.show()

pickup_location_fare_amount = trip_data.loc[:,['PULocationID','fare_amount']]
pickup_location_fare_amount

pickup_location_fare_amount_describe=pickup_location_fare_amount.groupby(['PULocationID']).agg(['median','count','max']).reset_index()
pickup_location_fare_amount_describe

pickup_location_fare_amount_describe_sorted=pickup_location_fare_amount_describe[("fare_amount","median")].sort_values(ascending=False)
sorted_index_list=pickup_location_fare_amount_describe_sorted.index
pickup_location_fare_amount_describe_sorted_True=pickup_location_fare_amount_describe.reindex(sorted_index_list).reset_index()
pickup_location_fare_amount_describe_sorted_True.drop(['index'],inplace=True,axis=1)
pickup_location_fare_amount_describe_sorted_True

"""* So from above plot we can observe that for busiest pickup location median fare_amount is quite low. Though the outliers for pickup location 184 is high.

This could be helpful in adjusting our revenue expectation based on putting our cabs in a given location because just choosing busy pickup locations for higher revenue won't work, we may have to choose locations taking into consideration both busy traffic and higher median fare_amount.

**DURATION EXPLORATION**

Here we will explore the duration of trip exploration with pickup hour of day.
"""

# plot box plot for duration for different hours of day
fig,ax = plt.subplots(figsize=(20,7))
# box plot using sns.boxplot x is 'trip_pickup_hour' and y is 'duration'
sns.boxplot(x = 'trip_pickup_hour', y='duration',data = trip_data,ax=ax)
ax.set_title('Box plot of trip_pickup hour with respect to trip duration')
sns.set()
plt.show()

"""Here again due to heavy outliers in duration data we are not able to observe the general graph. we might need to restrict our duration values to within 50min. """

# import pandas as pd
# import matplotlib.pyplot as plt 
# import numpy as np
# # import seaborn as sns
# restricted_duration=pd.read_parquet('/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/yellow_tripdata_Feb_2020_cleaned.parquet')
# restricted_duration.shape

# create restricted_duration dataframe with .loc on 'duration' column
restricted_duration= trip_data.loc[(trip_data['duration']<50) & (trip_data['duration']>0)]
restricted_duration.shape

fig,ax = plt.subplots(figsize=(20,7))
sns.boxplot(x = 'trip_pickup_hour', y='duration',data = restricted_duration,ax=ax)
ax.set_title('Box plot of trip_pickup hour with respect to trip duration')
sns.set()
plt.show()

"""Early morning hours of 5AM to 6AM have shorter duration trips

Let's also explore duration with respect to top pickup location.
"""

# plot box plots of duration for top 10 frequent pickup locations
for top_pickup_locID in top_10_frequent_pickup_locations:
    # create the new dataframe for each location using .loc on 'PULocationID' - pickup_locID_dataframe
    pickup_locID_dataframe = trip_data.loc[trip_data['PULocationID'] == top_pickup_locID]
    # print the median duration for the top_pickup_locID
    print('The median trip duration of trips taken from '+str(top_pickup_locID)+' is '+str(pickup_locID_dataframe['duration'].median()))
    fig,ax = plt.subplots(figsize=(6,6))
    # sns.boxplot of duration from the dataframe pickup_locID_dataframe
    sns.boxplot(pickup_locID_dataframe['duration'],ax=ax)
    # set_title
    ax.set_title('box plot of duration for pickup location '+ str(top_pickup_locID))
    sns.set()
    plt.show()

pickup_location_duration = trip_data.loc[:,['PULocationID','duration']]
pickup_location_duration

pickup_location_duration_describe=pickup_location_duration.groupby(['PULocationID']).agg(['median','count','max']).reset_index()
pickup_location_duration_describe

pickup_location_duration_describe_sorted=pickup_location_duration_describe[("duration","median")].sort_values(ascending=False)
sorted_index_list=pickup_location_duration_describe_sorted.index
pickup_location_duration_describe_sorted_True=pickup_location_duration_describe.reindex(sorted_index_list).reset_index()
pickup_location_duration_describe_sorted_True.drop(['index'],inplace=True,axis=1)
pickup_location_duration_describe_sorted_True

"""* Here again we can see for top 5 most frequent pickup locations 161,237,236,162,230, the duration value is less in comparison to other pickup locations. They have 11.6, 8.45, 8.58, 11.18, 11.167 median duration. 

* While 118, 99, 5, 86, and 44 are the top 5 locations with highest median duration.

* **Duration vs Fare amount**
"""

# trip_data=pd.read_parquet('/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/yellow_tripdata_Feb_2020_cleaned_before_routes.parquet')
# trip_data.head()

Duration_Fare_Taxes=trip_data.loc[:,["duration","fare_amount","total_taxes"]].copy()
Duration_Fare_Taxes.head()

column_fare_tax=["fare_amount","total_taxes"]

for feature in column_fare_tax:
    # removing the outliers from feature data
    feature_data_percentile = Duration_Fare_Taxes[feature].quantile(0.7)
    feature_data = Duration_Fare_Taxes.loc[Duration_Fare_Taxes[feature]< feature_data_percentile,feature]

    # removing the outliers from index data
    index_data_percentile = Duration_Fare_Taxes["duration"].quantile(0.7)
    index_data = Duration_Fare_Taxes.loc[Duration_Fare_Taxes["duration"]< index_data_percentile,"duration"]

    plt.figure(figsize=(12,6))
    sns.scatterplot(x=index_data,y=feature_data,data=Duration_Fare_Taxes)
    sns.set()
    plt.show()

"""* We can see that there is no clear trend is visible eaven after removing the outliers from the data
* Now let us use bin to divide the duration into category
* We already observed that most of the trips are within 5-16 minutes range, so keeping these values to create bins
"""

print(Duration_Fare_Taxes.shape)
Duration_Fare_Taxes=Duration_Fare_Taxes.loc[((Duration_Fare_Taxes["duration"]>=5) & (Duration_Fare_Taxes["duration"]<=16))].copy()
print(Duration_Fare_Taxes.shape)
Duration_Fare_Taxes.head()

bin=['5-6', '6-7', '7-8', '8-9', '9-10', '10-11','11-12','12-13','13-14','14-15','15-16']
Duration_Fare_Taxes['duration_bin']=pd.cut(Duration_Fare_Taxes["duration"], range(5,17,1),labels=bin)
Duration_Fare_Taxes.head()

for feature in column_fare_tax:
    plt.figure(figsize=(12,6))
    sns.barplot(x="duration_bin",y=feature,data=Duration_Fare_Taxes)
    sns.set()
    plt.show()

"""* It can be observed that as the duration of trip is increasing, fare amount is increasing
* While trip duration does not impact total_taxes

* **Fare amount and Total_taxes based on location and time**
"""

Fare_taxes_location_time = trip_data.loc[:,["fare_amount","total_taxes","PULocationID","DOLocationID","trip_pickup_hour","trip_dropoff_hour"]]
Fare_taxes_location_time

"""* We already found that the busiest location in terms of pickup are 161, 236 and 237 and busiest location in terms of dropoff are 161, 236 and 237.
* We also already found that peak hours of pick up and dropoff are 5-7PM. And busiest time is 6PM.
* So we will focus our analysis on these locations and hours only
"""

print(Fare_taxes_location_time.shape)
Fare_taxes_location_time=Fare_taxes_location_time.loc[Fare_taxes_location_time["PULocationID"].isin([161,236,237])]
print('After selecting pickup location',Fare_taxes_location_time.shape)
Fare_taxes_location_time=Fare_taxes_location_time.loc[Fare_taxes_location_time["DOLocationID"].isin([161,236,237])]
print('After selecting dropoff location',Fare_taxes_location_time.shape)
Fare_taxes_location_time=Fare_taxes_location_time.loc[Fare_taxes_location_time["trip_pickup_hour"].isin([17,18,19])]
print('After selecting pickup hour location',Fare_taxes_location_time.shape)
Fare_taxes_location_time=Fare_taxes_location_time.loc[Fare_taxes_location_time["trip_dropoff_hour"].isin([17,18,19])]
print('After selecting dropoff hour location',Fare_taxes_location_time.shape)
Fare_taxes_location_time

Fare_taxes_location_time.describe()

Fare_taxes_location_time.median()

fare_pickup_time_loca=Fare_taxes_location_time.groupby(["trip_pickup_hour","PULocationID"])['fare_amount'].median().reset_index()
fare_pickup_time_loca.head()

plt.figure(figsize=(20,8))
sns.barplot(x='trip_pickup_hour',y='fare_amount',hue='PULocationID',data=fare_pickup_time_loca)

fare_dropoff_time_loca=Fare_taxes_location_time.groupby(["trip_dropoff_hour","DOLocationID"])['fare_amount'].median().reset_index()
plt.figure(figsize=(20,8))
sns.barplot(x='trip_dropoff_hour',y='fare_amount',hue='DOLocationID',data=fare_dropoff_time_loca)

tax_pickup_time_loca=Fare_taxes_location_time.groupby(["trip_pickup_hour","PULocationID"])['total_taxes'].median().reset_index()
plt.figure(figsize=(20,8))
sns.barplot(x='trip_pickup_hour',y='total_taxes',hue='PULocationID',data=tax_pickup_time_loca)

tax_dropoff_time_loca=Fare_taxes_location_time.groupby(["trip_dropoff_hour","DOLocationID"])['total_taxes'].median().reset_index()
plt.figure(figsize=(20,8))
sns.barplot(x='trip_dropoff_hour',y='total_taxes',hue='DOLocationID',data=tax_dropoff_time_loca)

"""* Busiest location in terms of pickup and dropoff were 161, 236 and 237 in descending order, and maximum fare_amount is found for 161 location, while locations 237 and 236 have slight difference between the fare_Amount.
* Total_tax is found to be same most if the pickup and dropoff locations.

* **Duration based on location and time**
"""

Duration_location_time = trip_data.loc[:,["duration","PULocationID","DOLocationID","trip_pickup_hour","trip_dropoff_hour"]]
Duration_location_time.head()

print(Duration_location_time.shape)
Duration_location_time=Duration_location_time.loc[Duration_location_time["PULocationID"].isin([161,236,237])]
print('After selecting pickup location',Duration_location_time.shape)
Duration_location_time=Duration_location_time.loc[Duration_location_time["DOLocationID"].isin([161,236,237])]
print('After selecting dropoff location',Duration_location_time.shape)
Duration_location_time=Duration_location_time.loc[Duration_location_time["trip_pickup_hour"].isin([17,18,19])]
print('After selecting pickup hour location',Duration_location_time.shape)
Duration_location_time=Duration_location_time.loc[Duration_location_time["trip_dropoff_hour"].isin([17,18,19])]
print('After selecting dropoff hour location',Duration_location_time.shape)
Duration_location_time.head()

duration_pickup_time_loca=Duration_location_time.groupby(["trip_pickup_hour","PULocationID"])['duration'].median().reset_index()
plt.figure(figsize=(20,8))
sns.barplot(x='trip_pickup_hour',y='duration',hue='PULocationID',data=duration_pickup_time_loca)

duration_dropoff_time_loca=Duration_location_time.groupby(["trip_dropoff_hour","DOLocationID"])['duration'].median().reset_index()
plt.figure(figsize=(20,8))
sns.barplot(x='trip_dropoff_hour',y='duration',hue='DOLocationID',data=duration_dropoff_time_loca)

"""* Busiest location in terms of pickup and dropoff were 161, 236 and 237 in descending order, and maximum duration is found for 161 location, while locations 237 and 236 have slight difference between the duration.

## FINAL RESULTS FROM EDA

Following insights would be useful

* Fare_amount  - most of the fare amount is within 10 dollar value as is shown by the median value. Though there are some significant outliers, the maximum of which is 6052 dollars.

* Tip_amount - most of the tip amount is within 0-3 dollar as is shown by the median value. Though again here too we have outliers, the maximum of which is around 549 dollars. 

* Total_taxes - most of the total_taxes values is within 3-6 dollars as is shown by the median value. Though we have outliers in this case but it is not as signiificant as the case for tip and fare.

* Total_amount - most of the total_amount values is within 10-20 dollars as is shown by the median value. Again the outliers in this case seems mostly because of outliers in fare_amount.

* Tolls_amount - most of the tolls_amount value is 0 so it seems most of the trips do not have to pay for tolls. Still there are outliers and the maximum toll amount is 926 dollars

* Duration - most of the values in duration is within 5-16 minutes range as is shown by the median value. We do have some outliers which are beyond the range of 3000 minutes.

* Trip_distance - most of the trip_distance is within 1-2.5 miles value as is shown by the median. Outliers in this case seems mostly because of outliers in duration

* Credit card is the most preferred mode of payment followed by cash.

* Peak hour for the pick up and drop off is around evening from 5 to 7. The busiest time is 6PM

* There is less traffic during night times and only after 8AM in morning does the pickup and drop off starts picking up pace

* Saturday has the highest taxi uses while Sunday has the lowest taxi uses.

* From Monday to Friday taxi uses increases continuously

* Top 5 most frequent pickup locations are 161,237,236,162,230, and their duration value is less in comparison to other pickup locations. They have 11.6, 8.45, 8.58, 11.18, 11.167 median duration. 

* While 118, 99, 5, 86, and 44 are the top 5 locations with highest median duration.

* Four of the busiest routes are
    * 237-236
    * 236-236
    * 236-237
    * 264-264

* Mostly 1 or 2 passenger avail the cab. Group rides are less common.

* Pickup and dropoff hours do not have much affect on median fare_amount as median is almost same for all the hours.

* Evening from 4PM to 7PM have quite variable taxes and is a bit higher than other times, it may be due to higher traffic charges.

* Busiest pickup location the median fare_amount is a bit lower than other pickup locations.

* Early morning hours of 5AM to 6AM have shorter duration trips

* For top 5 most frequent pickup locations 161,237,236,162,230, the duration value is less in comparison to other pickup locations. They have 11.6, 8.45, 8.58, 11.18, 11.167 median duration. 

* While 118, 99, 5, 86, and 44 are the top 5 locations with highest median duration.

* It can be observed that as the duration of trip is increasing, fare amount is increasing for all peak hours

* While trip duration does not impact total_taxes for all peak hours

* Busiest location in terms of pickup and dropoff were 161, 236 and 237 in descending order, and maximum fare_amount is found for 161 location, while locations 237 and 236 have slight difference between the fare_Amount for all peak hours

* Total_tax is found to be same most if the pickup and dropoff locations for all peak hours

* Busiest location in terms of pickup and dropoff were 161, 236 and 237 in descending order, and maximum duration is found for 161 location, while locations 237 and 236 have slight difference between the duration for all peak hours

**#Rough Work**
"""

trip_data=pd.read_parquet('/content/drive/MyDrive/Onelearn_Data_Sciece_Course/EDA/Taxi_Project/Capstone_Project_Data/yellow_tripdata_Feb_2020_cleaned_before_routes.parquet')
zero_passenger_count=trip_data.loc[trip_data["passenger_count"]==0]
zero_passenger_count

print("trip day = ",zero_passenger_count["trip_day"].unique(),'\n')
print("Pickup location = ",zero_passenger_count["PULocationID"].unique(),'\n')
print("Drop off location = ",zero_passenger_count["DOLocationID"].unique(),'\n')
print("trip_pickup_hour = ",zero_passenger_count["trip_pickup_hour"].unique(),'\n')
print("trip_dropoff_hour = ",zero_passenger_count["trip_dropoff_hour"].unique(),'\n')
print("payment_type = ",zero_passenger_count["payment_type"].unique(),'\n')
print("payment_count = ",zero_passenger_count["payment_type"].value_counts())